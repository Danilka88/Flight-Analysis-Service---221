# Модуль AI-анализа через Ollama

Этот модуль представляет собой независимый FastAPI-сервис, отвечающий за взаимодействие с локально запущенной моделью Ollama для генерации аналитических сводок.

## API

### `POST /api/v1/ai/analyze`

-   **Описание**: Принимает данные от виджета, формирует промпт и запрашивает анализ у LLM.
-   **Тело запроса**: Соответствует модели `AnalyzeRequest` из `ollama_analyzer/models.py`.
-   **Логика**:
    1.  Пытается подключиться к Ollama и получить анализ.
    2.  В случае успеха, санирует HTML-ответ и возвращает его.
    3.  Если Ollama недоступна, возвращает заранее заготовленный демонстрационный ответ.

## Запуск

Модуль интегрируется в основное приложение `main.py`. Для запуска необходимо установить зависимости и запустить главный сервер:

```bash
pip install -r requirements.txt
uvicorn main:app --host 0.0.0.0 --port 8000
```
